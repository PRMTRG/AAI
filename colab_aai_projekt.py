# -*- coding: utf-8 -*-
"""AAI Projekt.ipynb

Automatically generated by Colaboratory.
"""

""" Install necessary packages. """

#!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
!pip3 install flair
!pip3 install sentence-transformers

""" Mount Google Drive folder. """

from google.colab import drive
drive.mount('/content/drive')
!ls -la /content/drive/MyDrive/AAI

""" Import Flair modules. """

from flair.data import Corpus
from flair.datasets import ClassificationCorpus
from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings, \
  TransformerDocumentEmbeddings, SentenceTransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer

""" Load corpus_1. """

label_type = 'subreddit'
aai_folder = '/content/drive/MyDrive/AAI/'

# corpus_1
# June 2021; subreddits: politics, AmItheAsshole
# removed: moderator comments, "[removed]", "[deleted]"
corpus_data_folder_1 = aai_folder + 'corp1'
corpus_1: Corpus = ClassificationCorpus(corpus_data_folder_1,
                                      test_file='test.txt',
                                      dev_file='dev.txt',
                                      train_file='train.txt',                                       
                                      label_type=label_type,
                                      )

""" Load corpus_2. """

label_type = 'subreddit'
aai_folder = '/content/drive/MyDrive/AAI/'

# corpus_2
# June 2021; subreddits: politics, AmItheAsshole, linux, wallstreetbets
# removed: 90% of politics, AmItheAsshole, wallstreetbets
# removed: moderator comments, "[removed]", "[deleted]"
corpus_data_folder_2 = aai_folder + 'corp2'
corpus_2: Corpus = ClassificationCorpus(corpus_data_folder_2,
                                      test_file='test.txt',
                                      dev_file='dev.txt',
                                      train_file='train.txt',                                       
                                      label_type=label_type,
                                      )

""" Downsample corpus_1. """

corpus_1 = corpus_1.downsample(0.03)

""" Downsample corpus_2. """

corpus_2 = corpus_2.downsample(0.2)

""" Create the label dictionary for corpus_1. """

label_dict_1 = corpus_1.make_label_dictionary(label_type=label_type)

""" Create the label dictionary for corpus_2. """

label_dict_2 = corpus_2.make_label_dictionary(label_type=label_type)

""" Train model on corpus_1 using transformer document embeddings with fine tuning. """

# initialize transformer document embeddings
document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)

# create the text classifier
classifier = TextClassifier(document_embeddings, label_dictionary=label_dict_1, label_type=label_type)

# initialize trainer
trainer = ModelTrainer(classifier, corpus_1)

# run training with fine-tuning
trainer.fine_tune('model3',
                  learning_rate=5.0e-5,
                  mini_batch_size=4,
                  max_epochs=10,
                  checkpoint=True,
                  )

""" Train model on corpus_1 using transformer document embeddings without fine tuning. """

# initialize transformer document embeddings
document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased')

# create the text classifier
classifier = TextClassifier(document_embeddings, label_dictionary=label_dict_1, label_type=label_type)

# initialize trainer
trainer = ModelTrainer(classifier, corpus_1)

# run training
trainer.train('model5',
                  learning_rate=5.0e-5,
                  mini_batch_size=4,
                  max_epochs=10,
                  embeddings_storage_mode='gpu', # can be used if not fine-tuning transformers 
                  checkpoint=True,
                  )

""" Train model on corpus_2 using sentence transformer document embeddings. """

# initialize sentence transformer document embeddings
document_embeddings = SentenceTransformerDocumentEmbeddings('bert-base-nli-mean-tokens')

# create the text classifier
classifier = TextClassifier(document_embeddings, label_dictionary=label_dict_2, label_type=label_type)

# initialize trainer
trainer = ModelTrainer(classifier, corpus_2)

# run training
trainer.train('model4',
                  learning_rate=5.0e-5,
                  mini_batch_size=4,
                  max_epochs=10,
                  embeddings_storage_mode='gpu', # can be used if not fine-tuning transformers 
                  checkpoint=True,
                  )
